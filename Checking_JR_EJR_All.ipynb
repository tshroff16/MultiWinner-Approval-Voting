{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7a11667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File_path:  pablulib2\\artificial_mechanical-turk_k-approval-3_.pb\n",
      "File_path:  pablulib2\\artificial_mechanical-turk_k-approval-6_.pb\n",
      "File_path:  pablulib2\\artificial_mechanical-turk_k-approval-7_.pb\n",
      "File_path:  pablulib2\\artificial_mechanical-turk_k-approval-8_.pb\n",
      "File_path:  pablulib2\\artificial_mechanical-turk_knapsack-3_.pb\n",
      "File_path:  pablulib2\\artificial_mechanical-turk_knapsack-6_.pb\n",
      "File_path:  pablulib2\\artificial_mechanical-turk_knapsack-7_.pb\n",
      "File_path:  pablulib2\\artificial_mechanical-turk_knapsack-8_.pb\n",
      "File_path:  pablulib2\\artificial_mechanical-turk_threshold-3_.pb\n",
      "File_path:  pablulib2\\artificial_mechanical-turk_threshold-6_.pb\n",
      "File_path:  pablulib2\\artificial_mechanical-turk_threshold-7_.pb\n",
      "File_path:  pablulib2\\artificial_mechanical-turk_threshold-8_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_166_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_179_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_212_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_252_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_267_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_285_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_287_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_288_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_289_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_304_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_309_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_324_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_332_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_335_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_358_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_417_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_489_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_490_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_491_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_492_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_509_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_515_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_522_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_523_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_524_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_525_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_588_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_604_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_605_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_613_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_619_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_621_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_622_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_628_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_631_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_644_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_645_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_646_.pb\n",
      "File_path:  pablulib2\\netherlands_amsterdam_647_.pb\n",
      "File_path:  pablulib2\\poland_gdynia_2020_.pb\n",
      "File_path:  pablulib2\\poland_gdynia_2020_babie-doly-large.pb\n",
      "File_path:  pablulib2\\poland_gdynia_2020_babie-doly-small.pb\n",
      "File_path:  pablulib2\\poland_gdynia_2020_chwarzno-wiczlino-large.pb\n",
      "File_path:  pablulib2\\poland_gdynia_2020_chwarzno-wiczlino-small.pb\n",
      "File_path:  pablulib2\\poland_gdynia_2020_chylonia-large.pb\n",
      "File_path:  pablulib2\\poland_gdynia_2020_chylonia-small.pb\n",
      "File_path:  pablulib2\\poland_gdynia_2020_cisowa-large.pb\n",
      "File_path:  pablulib2\\poland_gdynia_2020_cisowa-small.pb\n",
      "File_path:  pablulib2\\poland_gdynia_2020_dabrowa-large.pb\n",
      "File_path:  pablulib2\\poland_gdynia_2020_dabrowa-small.pb\n",
      "File_path:  pablulib2\\poland_gdynia_2020_dzialki-lesne-large.pb\n",
      "File_path:  pablulib2\\poland_gdynia_2020_dzialki-lesne-small.pb\n",
      "File_path:  pablulib2\\poland_gdynia_2020_grabowek-large.pb\n",
      "File_path:  pablulib2\\poland_gdynia_2020_grabowek-small.pb\n",
      "File_path:  pablulib2\\poland_gdynia_2020_kamienna-gora-large.pb\n",
      "File_path:  pablulib2\\poland_gdynia_2020_kamienna-gora-small.pb\n",
      "File_path:  pablulib2\\poland_gdynia_2020_karwiny-large.pb\n",
      "File_path:  pablulib2\\poland_lodz_2020_nad-nerem.pb\n",
      "File_path:  pablulib2\\poland_lodz_2020_nowosolna.pb\n",
      "File_path:  pablulib2\\poland_lodz_2020_nr-33.pb\n",
      "File_path:  pablulib2\\poland_lodz_2020_olechow-janow.pb\n",
      "File_path:  pablulib2\\poland_lodz_2020_piastow-kurak.pb\n",
      "File_path:  pablulib2\\poland_lodz_2020_radogoszcz.pb\n",
      "File_path:  pablulib2\\poland_lodz_2020_retkinia-zachod-smulsko.pb\n",
      "File_path:  pablulib2\\poland_lodz_2020_rokicie.pb\n",
      "File_path:  pablulib2\\poland_lodz_2020_ruda.pb\n",
      "File_path:  pablulib2\\poland_lodz_2020_srodmiescie-katedralna.pb\n",
      "File_path:  pablulib2\\poland_lodz_2020_srodmiescie-wschod.pb\n",
      "File_path:  pablulib2\\poland_lodz_2020_stare-polesie.pb\n",
      "File_path:  pablulib2\\poland_lodz_2020_stary-widzew.pb\n",
      "File_path:  pablulib2\\poland_lodz_2020_stoki-sikawa-podgorze.pb\n",
      "File_path:  pablulib2\\poland_lodz_2020_teofilow-wielkopolska.pb\n",
      "File_path:  pablulib2\\poland_lodz_2020_widzew-wschod.pb\n",
      "File_path:  pablulib2\\poland_lodz_2020_wiskitno.pb\n",
      "File_path:  pablulib2\\poland_lodz_2020_wzniesien-lodzkich.pb\n",
      "File_path:  pablulib2\\poland_lodz_2020_zarzew.pb\n",
      "File_path:  pablulib2\\poland_lodz_2020_zdrowie-mania.pb\n",
      "File_path:  pablulib2\\poland_lodz_2020_zlotno.pb\n",
      "File_path:  pablulib2\\poland_lodz_2022_.pb\n",
      "File_path:  pablulib2\\poland_lodz_2022_andrzejow.pb\n",
      "File_path:  pablulib2\\poland_lodz_2022_baluty-centrum.pb\n",
      "File_path:  pablulib2\\poland_lodz_2022_baluty-doly.pb\n",
      "File_path:  pablulib2\\poland_lodz_2022_baluty-zachodnie.pb\n",
      "File_path:  pablulib2\\poland_lodz_2022_chojny-dabrowa.pb\n",
      "File_path:  pablulib2\\poland_lodz_2022_chojny.pb\n",
      "File_path:  pablulib2\\poland_lodz_2022_dolina-lodki.pb\n",
      "File_path:  pablulib2\\poland_lodz_2022_gorniak.pb\n",
      "File_path:  pablulib2\\poland_lodz_2022_im-jozefa-montwilla-mireckiego.pb\n",
      "File_path:  pablulib2\\poland_lodz_2022_julianow-marysin-rogi.pb\n",
      "File_path:  pablulib2\\poland_lodz_2022_karolew-retkinia-wschod.pb\n",
      "File_path:  pablulib2\\poland_lodz_2022_katedralna.pb\n",
      "File_path:  pablulib2\\poland_lodz_2022_koziny.pb\n",
      "File_path:  pablulib2\\poland_lodz_2022_lagiewniki.pb\n",
      "File_path:  pablulib2\\poland_lodz_2022_lublinek-pienista.pb\n",
      "File_path:  pablulib2\\poland_lodz_2022_mileszki.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_marysin-polnocny.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_marysin-poludniowy.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_miedzeszyn.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_miedzylesie.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_mlynow.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_nadwisle.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_niskie-okecie.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_nowa-praga-z-pelcowizna.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_nowe-wlochy.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_nowolipki-powazki.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_ochota.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_piaski.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_plac-wojska-polskiego.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_praga-polnoc.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_praga-poludnie.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_przyczolek-grochowski.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_radiowo-wolka-weglowa-placowka-huta.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_radosc.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_rakowiec.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_rejon-a.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_rejon-b.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_rejon-c.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_rejon-d.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_rejon-e.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_rejon-f.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_rembertow.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_ruda-marymont-las-bielanski.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_sadul.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_sady-zoliborskie-zatrasie.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_saska-kepa.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_sawa.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_slodowiec.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_srodmiescie.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_stara-milosna.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_stara-ochota.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_stara-praga-i-szmulowizna-z-michalowem.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_stare-bielany.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_stare-wlochy.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_subunit-wawer.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_szczesliwice.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_tarchomin-nowodwory-piekielko.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_targowek-fabryczny-elsnerow-i-utrata.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_targowek-mieszkaniowy.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_ulrychow-odolany.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_ursus-polnocny.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_ursus-poludniowy.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_ursynow-wysoki-polnocny.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_ursynow-wysoki-poludniowy.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_ursynow.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_wars.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_wawer.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_wawrzyszew.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_wilanow-obszar-i.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_wilanow-obszar-ii.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_wrzeciono-mlociny.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_wysokie-okecie.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_zacisze.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_zerzen.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_zielona-grzybowa.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_zielony-ursynow.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_zoliborz-centralny.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2017_zoliborz-poludniowy-powazki.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2018_aleksandrow.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2018_anin.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2018_bialoleka-obszar-1.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2018_bialoleka-obszar-2.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2018_bialoleka-obszar-3.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2018_subunit-wawer.pb\n",
      "File_path:  pablulib2\\poland_warszawa_2018_szczesliwice.pb\n",
      "Summary for all files:\n",
      "    k  jr_1_count  ejr_1_count  jr_percentage  ejr_percentage\n",
      "0   9        10.0         1759          100.0         17590.0\n",
      "1  10        20.0         1758          200.0         17580.0\n",
      "2   1         0.0         1760            0.0         17600.0\n",
      "3   2         0.0         1760            0.0         17600.0\n",
      "4   3         0.0         1760            0.0         17600.0\n",
      "5   4         0.0         1760            0.0         17600.0\n",
      "6   5         0.0         1760            0.0         17600.0\n",
      "7   6         0.0         1760            0.0         17600.0\n",
      "8   7         0.0         1760            0.0         17600.0\n",
      "9   8         0.0         1760            0.0         17600.0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from itertools import chain, combinations\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Define functions for JR, EJR, and other operations\n",
    "\n",
    "def powerset(iterable, max_size):\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(1, min(len(s)+1, max_size+1)))\n",
    "\n",
    "def subset_of_len_l(C_modified, l):\n",
    "    return set(combinations(C_modified, l))\n",
    "\n",
    "def create_set_W(df, W):\n",
    "    return set(df.head(W)['Value'])\n",
    "\n",
    "def JR(non_winners, approval_dict, W_set, k, n):\n",
    "    start_time = time.time()  # Record start time\n",
    "    duration = 0  # Initialize duration\n",
    "\n",
    "    counts = {}\n",
    "    breaks_jr = False\n",
    "\n",
    "    for non_winner in non_winners:\n",
    "        counts[non_winner] = 0\n",
    "        for key, value in approval_dict.items():\n",
    "            if non_winner in value and not set(W_set).intersection(value):\n",
    "                counts[non_winner] += 1\n",
    "                if counts[non_winner] >= n / k:\n",
    "                    breaks_jr = True\n",
    "                    break\n",
    "        if breaks_jr:\n",
    "            duration = time.time() - start_time\n",
    "            return 1, duration  # Return a tuple\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    return 0, duration  # Return a tuple\n",
    "\n",
    "\n",
    "def EJR(approval_lists, winners, k, n, C,l):\n",
    "    start_time = time.time()  # Record start time\n",
    "    \n",
    "    C_modified = [candidate for candidate in C if greedy_list.loc[greedy_list['Value'] == candidate, 'Count'].iloc[0] >= l * n / k]\n",
    "    S = subset_of_len_l(C_modified, l)\n",
    "\n",
    "    for s in S:\n",
    "        count = 0\n",
    "        for approval_list in approval_lists.values():\n",
    "            if set(s).issubset(approval_list):\n",
    "                if abs(len(set(approval_list) & set(winners))) < l:\n",
    "                    count += 1\n",
    "        if count >= l * n / k:\n",
    "            end_time = time.time()  # Record end time\n",
    "            duration = end_time - start_time  # Calculate duration\n",
    "            return 0, duration\n",
    "                \n",
    "    end_time = time.time()  # Record end time\n",
    "    duration = end_time - start_time  # Calculate duration\n",
    "    return 1, duration\n",
    "\n",
    "# Directory where input files are stored\n",
    "input_directory = 'pablulib2'\n",
    "\n",
    "# List of file paths\n",
    "file_paths = []\n",
    "\n",
    "# Read all file paths from the directory\n",
    "for root, dirs, files in os.walk(input_directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".pb\"):\n",
    "            file_paths.append(os.path.join(root, file))\n",
    "\n",
    "# Directory where result files will be saved\n",
    "results_directory = 'results_pabulib'\n",
    "os.makedirs(results_directory, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "summary_results = []\n",
    "\n",
    "# Iterate over each file path\n",
    "for file_path in file_paths:\n",
    "    print(\"File_path: \", file_path)\n",
    "    with open(file_path, 'r', newline='', encoding=\"utf-8\") as csvfile:\n",
    "        meta = {}\n",
    "        projects = {}\n",
    "        votes = {}\n",
    "        section = \"\"\n",
    "        header = []\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        for row in reader:\n",
    "            if str(row[0]).strip().lower() in [\"meta\", \"projects\", \"votes\"]:\n",
    "                section = str(row[0]).strip().lower()\n",
    "                header = next(reader)\n",
    "            elif section == \"meta\":\n",
    "                meta[row[0]] = row[1].strip()\n",
    "            elif section == \"projects\":\n",
    "                projects[row[0]] = {}\n",
    "                for it, key in enumerate(header[1:]):\n",
    "                    projects[row[0]][key.strip()] = row[it+1].strip()\n",
    "            elif section == \"votes\":\n",
    "                votes[row[0]] = {}\n",
    "                for it, key in enumerate(header[1:]):\n",
    "                    votes[row[0]][key.strip()] = row[it+1].strip()\n",
    "\n",
    "    # Extract required data and perform operations as before...\n",
    "    \n",
    "    C = set(projects.keys())\n",
    "    V = set(votes.keys())\n",
    "    n = len(V)\n",
    "    approval_dict = {key: set(value['vote'].split(',')) for key, value in votes.items()}\n",
    "\n",
    "    lengths = [len(value) for value in approval_dict.values()]\n",
    "\n",
    "    # Calculate the average length\n",
    "    average_length = sum(lengths) / len(lengths)\n",
    "\n",
    "    #print(\"Average length of values:\", average_length)\n",
    "\n",
    "    max_length = min(len(value) for value in approval_dict.values())\n",
    "\n",
    "    # Creating a DataFrame with the projects and the number of votes they got\n",
    "    all_values = set()\n",
    "    for value_set in approval_dict.values():\n",
    "        all_values.update(value_set)\n",
    "\n",
    "    # Count occurrences of each value in the entire dictionary\n",
    "    count_dict = Counter()\n",
    "    for value_set in approval_dict.values():\n",
    "        count_dict.update(value_set)\n",
    "\n",
    "    # Create a list with unique values and their counts\n",
    "    result_list = [[value, count_dict[value]] for value in all_values]\n",
    "\n",
    "    greedy_list = pd.DataFrame(result_list, columns=['Value', 'Count'])\n",
    "    greedy_list = greedy_list.sort_values(by='Count', ascending=False)\n",
    "\n",
    "    W_range = range(1, 11)\n",
    "    results_single_file = []\n",
    "\n",
    "    # Iterate over different values of W\n",
    "    for l in W_range:\n",
    "        for W_value in W_range:\n",
    "            W_set = create_set_W(greedy_list, W_value)\n",
    "            k = len(W_set)\n",
    "            non_winners = C - W_set\n",
    "\n",
    "            # Check for JR\n",
    "            result1, duration1 = JR(non_winners, approval_dict, W_set, k, n)\n",
    "\n",
    "            # Check for EJR\n",
    "            result2, duration2 = EJR(approval_dict, W_set, k, n, C, l)\n",
    "\n",
    "            results_single_file.append((l, W_value, result1, duration1, result2, duration2))\n",
    "\n",
    "    results_df = pd.DataFrame(results_single_file, columns=['l', 'k', 'JR', 'JR Duration', 'EJR', 'EJR Duration'])\n",
    "\n",
    "    # Save results to a CSV file\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0] + '_results.csv'\n",
    "    results_path = os.path.join(results_directory, file_name)\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "\n",
    "    # Append results for each file path to the results list\n",
    "    results.append(results_df)\n",
    "\n",
    "# Concatenate results from all files into one dataframe\n",
    "all_results_df = pd.concat(results)\n",
    "\n",
    "# Calculate percentage of distinct values of column 'k' where 'jr' column is 1\n",
    "jr_column_1_count = all_results_df[all_results_df['JR'] == 1].groupby('k').size().reset_index(name='jr_1_count')\n",
    "\n",
    "# Calculate percentage of distinct values of column 'k' where 'ejr' column is 1\n",
    "ejr_column_1_count = all_results_df[all_results_df['EJR'] == 1].groupby('k').size().reset_index(name='ejr_1_count')\n",
    "\n",
    "# Merge the counts for 'jr' and 'ejr' by 'k'\n",
    "merged_counts = pd.merge(jr_column_1_count, ejr_column_1_count, on='k', how='outer').fillna(0)\n",
    "\n",
    "# Calculate the total distinct values of 'k' across all files\n",
    "# total_distinct_k_values = len(set(merged_counts['k']))\n",
    "total_distinct_k_values = len(set(merged_counts['k']))\n",
    "# Calculate percentage of 'jr' and 'ejr' being 1 for each 'k'\n",
    "merged_counts['jr_percentage'] = (merged_counts['jr_1_count'] / total_distinct_k_values) * 100\n",
    "merged_counts['ejr_percentage'] = (merged_counts['ejr_1_count'] / total_distinct_k_values) * 100\n",
    "\n",
    "# Display the summary\n",
    "print(\"Summary for all files:\")\n",
    "print(merged_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3dcf61c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Installing collected packages: tabulate\n",
      "Successfully installed tabulate-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\divya\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5910d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l</th>\n",
       "      <th>k</th>\n",
       "      <th>JR</th>\n",
       "      <th>JR Duration</th>\n",
       "      <th>EJR</th>\n",
       "      <th>EJR Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002992</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>1</td>\n",
       "      <td>0.006980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>1</td>\n",
       "      <td>0.007978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001995</td>\n",
       "      <td>1</td>\n",
       "      <td>0.008076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   l  k  JR  JR Duration  EJR  EJR Duration\n",
       "0  1  1   0     0.000998    1      0.004781\n",
       "1  1  2   0     0.002992    1      0.003989\n",
       "2  1  3   0     0.001996    1      0.006980\n",
       "3  1  4   0     0.000998    1      0.007978\n",
       "4  1  5   0     0.001995    1      0.008076"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63c79580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_distinct_k_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d1fe63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f62fc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from itertools import chain, combinations\n",
    "from tabulate import tabulate\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Define functions for JR, EJR, and other operations\n",
    "\n",
    "def powerset(iterable, max_size):\n",
    "    s = list(iterable)\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(1, min(len(s)+1, max_size+1)))\n",
    "\n",
    "def subset_of_len_l(C_modified, l):\n",
    "    return set(combinations(C_modified, l))\n",
    "\n",
    "def create_set_W(df, W):\n",
    "    return set(df.head(W)['Value'])\n",
    "\n",
    "def JR(non_winners, approval_dict, W_set, k, n):\n",
    "    start_time = time.time()  # Record start time\n",
    "    duration = 0  # Initialize duration\n",
    "\n",
    "    counts = {}\n",
    "    breaks_jr = False\n",
    "\n",
    "    for non_winner in non_winners:\n",
    "        counts[non_winner] = 0\n",
    "        for key, value in approval_dict.items():\n",
    "            if non_winner in value and not set(W_set).intersection(value):\n",
    "                counts[non_winner] += 1\n",
    "                if counts[non_winner] >= n / k:\n",
    "                    breaks_jr = True\n",
    "                    break\n",
    "    if breaks_jr:\n",
    "        duration = time.time() - start_time\n",
    "        return 0, duration  # Return a tuple\n",
    "    else:\n",
    "        duration = time.time() - start_time\n",
    "        return 1, duration  # Return a tuple\n",
    "\n",
    "\n",
    "def EJR(approval_lists, winners, k, n, C,l):\n",
    "    start_time = time.time()  # Record start time\n",
    "    \n",
    "    C_modified = [candidate for candidate in C if greedy_list.loc[greedy_list['Value'] == candidate, 'Count'].iloc[0] >= l * n / k]\n",
    "    S = subset_of_len_l(C_modified, l)\n",
    "\n",
    "    for s in S:\n",
    "        count = 0\n",
    "        for approval_list in approval_lists.values():\n",
    "            if set(s).issubset(approval_list):\n",
    "                if abs(len(set(approval_list) & set(winners))) < l:\n",
    "                    count += 1\n",
    "        if count >= l * n / k:\n",
    "            end_time = time.time()  # Record end time\n",
    "            duration = end_time - start_time  # Calculate duration\n",
    "            return 0, duration\n",
    "                \n",
    "    end_time = time.time()  # Record end time\n",
    "    duration = end_time - start_time  # Calculate duration\n",
    "    return 1, duration\n",
    "\n",
    "# Directory where input files are stored\n",
    "input_directory = 'pabulib_files'\n",
    "\n",
    "# List of file paths\n",
    "file_paths = []\n",
    "\n",
    "# Read all file paths from the directory\n",
    "for root, dirs, files in os.walk(input_directory):\n",
    "    for file in files:\n",
    "        if file.endswith(\".pb\"):\n",
    "            file_paths.append(os.path.join(root, file))\n",
    "\n",
    "# Directory where result files will be saved\n",
    "results_directory = 'results_pabulib'\n",
    "os.makedirs(results_directory, exist_ok=True)\n",
    "\n",
    "results = []\n",
    "summary_results = []\n",
    "\n",
    "# Iterate over each file path\n",
    "for file_path in file_paths:\n",
    "    with open(file_path, 'r', newline='', encoding=\"utf-8\") as csvfile:\n",
    "        meta = {}\n",
    "        projects = {}\n",
    "        votes = {}\n",
    "        section = \"\"\n",
    "        header = []\n",
    "        reader = csv.reader(csvfile, delimiter=';')\n",
    "        for row in reader:\n",
    "            if str(row[0]).strip().lower() in [\"meta\", \"projects\", \"votes\"]:\n",
    "                section = str(row[0]).strip().lower()\n",
    "                header = next(reader)\n",
    "            elif section == \"meta\":\n",
    "                meta[row[0]] = row[1].strip()\n",
    "            elif section == \"projects\":\n",
    "                projects[row[0]] = {}\n",
    "                for it, key in enumerate(header[1:]):\n",
    "                    projects[row[0]][key.strip()] = row[it+1].strip()\n",
    "            elif section == \"votes\":\n",
    "                votes[row[0]] = {}\n",
    "                for it, key in enumerate(header[1:]):\n",
    "                    votes[row[0]][key.strip()] = row[it+1].strip()\n",
    "\n",
    "    # Extract required data and perform operations as before...\n",
    "    \n",
    "    C = set(projects.keys())\n",
    "    V = set(votes.keys())\n",
    "    n = len(V)\n",
    "    approval_dict = {key: set(value['vote'].split(',')) for key, value in votes.items()}\n",
    "\n",
    "    lengths = [len(value) for value in approval_dict.values()]\n",
    "\n",
    "    # Calculate the average length\n",
    "    average_length = sum(lengths) / len(lengths)\n",
    "\n",
    "    #print(\"Average length of values:\", average_length)\n",
    "\n",
    "    max_length = min(len(value) for value in approval_dict.values())\n",
    "\n",
    "    # Creating a DataFrame with the projects and the number of votes they got\n",
    "    all_values = set()\n",
    "    for value_set in approval_dict.values():\n",
    "        all_values.update(value_set)\n",
    "\n",
    "    # Count occurrences of each value in the entire dictionary\n",
    "    count_dict = Counter()\n",
    "    for value_set in approval_dict.values():\n",
    "        count_dict.update(value_set)\n",
    "\n",
    "    # Create a list with unique values and their counts\n",
    "    result_list = [[value, count_dict[value]] for value in all_values]\n",
    "\n",
    "    greedy_list = pd.DataFrame(result_list, columns=['Value', 'Count'])\n",
    "    greedy_list = greedy_list.sort_values(by='Count', ascending=False)\n",
    "\n",
    "    W_range = range(1, 11)\n",
    "    results_single_file = []\n",
    "\n",
    "    # Iterate over different values of W\n",
    "    for l in W_range:\n",
    "        for W_value in W_range:\n",
    "            W_set = create_set_W(greedy_list, W_value)\n",
    "            k = len(W_set)\n",
    "            non_winners = C - W_set\n",
    "\n",
    "            # Check for JR\n",
    "            result1, duration1 = JR(non_winners, approval_dict, W_set, k, n)\n",
    "\n",
    "            # Check for EJR\n",
    "            result2, duration2 = EJR(approval_dict, W_set, k, n, C, l)\n",
    "\n",
    "            results_single_file.append((l, W_value, result1, duration1, result2, duration2))\n",
    "\n",
    "    results_df = pd.DataFrame(results_single_file, columns=['l', 'k', 'JR', 'JR Duration', 'EJR', 'EJR Duration'])\n",
    "\n",
    "    # Save results to a CSV file\n",
    "    file_name = os.path.splitext(os.path.basename(file_path))[0] + '_results.csv'\n",
    "    results_path = os.path.join(results_directory, file_name)\n",
    "    results_df.to_csv(results_path, index=False)\n",
    "\n",
    "    # Append results for each file path to the results list\n",
    "    results.append(results_df)\n",
    "\n",
    "# Concatenate results from all files into one dataframe\n",
    "all_results_df = pd.concat(results)\n",
    "\n",
    "# Calculate percentage of distinct values of column 'k' where 'jr' column is 1\n",
    "jr_column_1_count = all_results_df[all_results_df['JR'] == 1].groupby('k').size().reset_index(name='jr_1_count')\n",
    "\n",
    "# Calculate percentage of distinct values of column 'k' where 'ejr' column is 1\n",
    "ejr_column_1_count = all_results_df[all_results_df['EJR'] == 1].groupby('k').size().reset_index(name='ejr_1_count')\n",
    "\n",
    "# Merge the counts for 'jr' and 'ejr' by 'k'\n",
    "merged_counts = pd.merge(jr_column_1_count, ejr_column_1_count, on='k', how='outer').fillna(0)\n",
    "\n",
    "# Calculate the total distinct values of 'k' across all files\n",
    "# total_distinct_k_values = len(set(merged_counts['k']))\n",
    "total_distinct_k_values = 1370\n",
    "# Calculate percentage of 'jr' and 'ejr' being 1 for each 'k'\n",
    "merged_counts['jr_percentage'] = (merged_counts['jr_1_count'] / total_distinct_k_values) * 100\n",
    "merged_counts['ejr_percentage'] = (merged_counts['ejr_1_count'] / total_distinct_k_values) * 100\n",
    "\n",
    "# Display the summary\n",
    "print(\"Summary for all files:\")\n",
    "print(merged_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0880aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l</th>\n",
       "      <th>k</th>\n",
       "      <th>JR</th>\n",
       "      <th>JR Duration</th>\n",
       "      <th>EJR</th>\n",
       "      <th>EJR Duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.047889</td>\n",
       "      <td>1</td>\n",
       "      <td>0.016202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.105674</td>\n",
       "      <td>1</td>\n",
       "      <td>0.024516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.151017</td>\n",
       "      <td>1</td>\n",
       "      <td>0.027596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.131291</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.089854</td>\n",
       "      <td>1</td>\n",
       "      <td>0.090339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.475794</td>\n",
       "      <td>1</td>\n",
       "      <td>0.070637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0.606285</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.590395</td>\n",
       "      <td>1</td>\n",
       "      <td>0.106048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.687107</td>\n",
       "      <td>1</td>\n",
       "      <td>0.100430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0.587982</td>\n",
       "      <td>1</td>\n",
       "      <td>0.093890</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13700 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     l   k  JR  JR Duration  EJR  EJR Duration\n",
       "0    1   1   1     0.047889    1      0.016202\n",
       "1    1   2   1     0.105674    1      0.024516\n",
       "2    1   3   1     0.151017    1      0.027596\n",
       "3    1   4   1     0.131291    1      0.106275\n",
       "4    1   5   1     0.089854    1      0.090339\n",
       "..  ..  ..  ..          ...  ...           ...\n",
       "95  10   6   1     0.475794    1      0.070637\n",
       "96  10   7   1     0.606285    1      0.100729\n",
       "97  10   8   1     0.590395    1      0.106048\n",
       "98  10   9   1     0.687107    1      0.100430\n",
       "99  10  10   1     0.587982    1      0.093890\n",
       "\n",
       "[13700 rows x 6 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda5ec5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
